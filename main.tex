\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{natbib}

\title{FitCoachAR: Real-Time Adaptive Exercise Coaching via Pose Estimation and AR Feedback}
\author{Yangyang Zhang \and Maximilian Fuchs \and Seyedmohamad Mirhoseininejad}
\date{CAS 772: Mobile Data Analytics}

\begin{document}

\maketitle

\section{Introduction}
Home and gym fitness applications increasingly employ AI-based pose estimation to count exercise repetitions, yet most still lack real-time, adaptive coaching on movement quality. Early research systems such as Pose Trainer \cite{chen2020pose} and AIFit \cite{fieraru2021aifit} demonstrate that 3D pose analysis can detect posture errors and evaluate performance, but their pipelines remain offline, relying on recorded videos and computationally heavy 3D reconstruction. They provide limited personalization and cannot adapt feedback dynamically during a workout. Recent work such as ARFit \cite{mandic2023arfit} integrates augmented-reality visualization to guide users through motion sequences, but still applies generic thresholds and lacks quantitative evaluation of performance consistency.

Commercial applications (e.g., Top Pushup \cite{toppushup}, QuickPose \cite{quickpose}) have popularized real-time motion counting and form tracking using smartphone cameras. However, they mainly focus on rep detection and simple form classification without deeper analysis—such as distinguishing good vs. bad repetitions, assessing tempo stability, or tracking per-joint improvement over time. Moreover, most commercial apps rely on fixed thresholds and provide limited adaptive or personalized feedback.

FitCoachAR aims to bridge the gap between academic models and consumer applications by providing a lightweight, real-time, and adaptive coaching system. It monitors exercises via 2D pose estimation, detects common form errors, and delivers feedback through augmented-reality overlays and coach-style natural language cues. The system personalizes its critique level through a short calibration phase and produces both live corrections and post-session summaries.

This project addresses three major motivations:
\begin{itemize}
    \item \textbf{Accessibility}: Eliminate the dependency on motion-capture equipment and high-end GPUs.
    \item \textbf{Personalization}: Adapt thresholds and critique sensitivity ($\delta$) to each user.
    \item \textbf{Interactivity}: Transform static, after-exercise evaluation into continuous, real-time feedback that enhances motivation and engagement.
\end{itemize}

\section{Related Work}
\subsection{Academic Systems}
Pose Trainer (2020) \cite{chen2020pose} applied rule-based analysis of 2D skeletons from OpenPose \cite{cao2017realtime} to classify push-ups and squats. While effective for offline evaluation, it operates on recorded videos and uses fixed thresholds, providing no real-time correction or personalization.

AIFit (CVPR 2021) \cite{fieraru2021aifit} introduced the Fit3D motion-capture dataset and a 3D feedback model capable of joint-level error localization using a deviation parameter ($\eta$) to control strictness. However, it depends on full-sequence 3D reconstruction (MubyNet) and produces template-based text feedback, making it computationally expensive and less adaptive.

ARFit (IMWUT 2023) \cite{mandic2023arfit} added mobile AR overlays for pose visualization, showing that spatial feedback improves exercise learning. Yet, its thresholds remain generic and it lacks quantitative analytics such as repetition consistency or tempo tracking, focusing mainly on visual guidance.

\subsection{Commercial Applications}
Apps like Top Pushup \cite{toppushup} and QuickPose \cite{quickpose} popularized real-time repetition counting using smartphone cameras. Their analysis remains binary—labeling repetitions as correct or incorrect—without distinguishing motion quality, tempo stability, or per-joint improvements. They also rely on static thresholds and simple chatbot-style feedback rather than adaptive, natural-language coaching.

\subsection{Gap Summary}
Across both research and consumer systems, key limitations persist:
\begin{itemize}
    \item Offline or delayed feedback (Pose Trainer, AIFit).
    \item Lack of personalization (global thresholds or $\delta$ not user-specific).
    \item Shallow analysis (no holistic quality metrics).
    \item Limited interactivity (no context-aware language feedback).
\end{itemize}

FitCoachAR addresses these gaps through:
\begin{itemize}
    \item Real-time 2D tracking and online calibration.
    \item Adaptive deviation sensitivity ($\eta$) for user-specific tolerance.
    \item AR-based spatial feedback.
    \item LLM-driven natural coaching for motivating, context-aware guidance.
\end{itemize}
This integration combines AIFit’s interpretability with ARFit’s usability, achieving real-time, personalized exercise feedback on consumer hardware.

\section{Methodology}

\subsection{Pose Acquisition}
We employ MediaPipe Pose \cite{lugaresi2019mediapipe} for real-time keypoint extraction (33 2D joints). The stream is smoothed using a One-Euro filter to reduce jitter and ensure sub-100 ms latency.

\subsection{Online Repetition Segmentation}
Following AIFit’s unsupervised approach, motion periodicity is analyzed from key joint angles (elbow, knee, hip). A simplified online state machine detects phase transitions—descent, bottom, ascent—using derivative sign changes with hysteresis. Each full cycle is labeled as one repetition.

\subsection{Feature Computation and Calibration}
From each repetition we compute angular features:
\begin{itemize}
    \item Active joints: elbows, knees, shoulders (max, min, correlation).
    \item Passive joints: spine and pelvis (mean, standard deviation).
\end{itemize}

During calibration, each key joint or motion phase $i$ is analyzed over three ``best-form'' repetitions:
The mean joint angle is
\begin{equation}
    \bar{U}_i = \frac{1}{3} \sum_{r=1}^{3} \theta_i^{(r)}
\end{equation}

Its deviation percentage from a canonical target $S_i$ is
\begin{equation}
    \eta_i = \frac{\bar{U}_i - S_i}{S_i}
\end{equation}

The pair $(\bar{U}_i, \eta_i)$ forms the personalized baseline and offset for that user.
During runtime, for each joint or phase, the system:
\begin{enumerate}
    \item Measures the current angle $\theta_i$
    \item Computes percentage deviation $e_i = \frac{|\theta_i - \bar{U}_i|}{|\bar{U}_i|}$
\end{enumerate}

A manually set critic parameter $\delta$ determines grading bands:
\begin{itemize}
    \item Good: $e_i < \delta$
    \item Relatively good: $\delta \le e_i < 1.2 \delta$
    \item Bad: $e_i \ge 1.2 \delta$
\end{itemize}

Repetition-level and session-level scores aggregate these joint/phase grades to summarize overall performance.



% SECTION 4.5 EXCLUDED AS REQUESTED
\input{formscript_section}

\subsection{Real-Time Visual and Rule-Based Feedback}
To maintain sub-100 ms latency during exercise, real-time feedback uses deterministic rule-based templates combined with AR visualization:
\begin{itemize}
    \item Template-based corrections: pre-defined feedback phrases mapped to specific error conditions (e.g., ``Lower your hips'' when pelvis deviation exceeds a threshold).
    \item AR overlay: skeleton lines, target ``shadow'' poses, arrows toward ideal joint positions, and colored angle sectors (green = within band, red = error).
    \item Audio cues (optional): brief beeps or spoken keywords for critical errors during high-intensity phases.
\end{itemize}
This approach ensures immediate, actionable feedback without network or computation delays, keeping users engaged in the flow state of exercise.

\subsection{LLM-Driven Post-Session Analysis and Summary}
After completing a workout session, accumulated performance data is processed by a Large Language Model to generate comprehensive, personalized coaching feedback.

\paragraph{Input construction}
Each session produces a detailed record such as:
\begin{verbatim}
{
  "exercise": "push_up",
  "total_reps": 15,
...
  "critic_level": 0.2
}
\end{verbatim}

\paragraph{LLM prompt design}
A structured system prompt guides the model to act as a professional coach:
``You are an expert fitness coach reviewing a completed workout session... Total response: 120–150 words.''

\paragraph{Output example}
``Excellent consistency on your 15 push-ups! Your tempo and range of motion were strong throughout...''

\paragraph{Benefits of post-session LLM use}
\begin{itemize}
    \item Richer context: full session history enables pattern detection.
    \item Biomechanical reasoning: LLM can explain why an error matters.
    \item Progression tracking: compare across sessions to show improvement.
    \item No latency constraints: 2–3 second generation is acceptable after workout.
    \item Cost-effective: one API call per session vs. hundreds during exercise.
\end{itemize}

Session statistics complement the LLM narrative with quantitative metrics: Total repetitions and success ratio, Per-joint error heatmap, Average tempo, range of motion, and phase durations, Personalized deviation parameter $\eta$ adjustments for next session.

\section{Evaluation Plan}
\paragraph{Metrics and targets}
\begin{itemize}
    \item Segmentation IoU: overlap of detected vs. manual rep boundaries; target $\ge 0.70$.
    \item Latency: end-to-end delay (camera $\rightarrow$ feedback); target $< 100$ ms.
    \item Error detection F1: accuracy of error detection vs. labeled data; target $> 0.80$.
    \item User study: 5 participants rate clarity and usefulness (1–5 Likert); target $\ge 4.0$ average.
    \item Personalization gain: precision improvement after calibration; target $+10\%$.
    \item Summary usefulness: user rating of LLM feedback (1–5 Likert); target $\ge 4.2$ average.
    \item Summary relevance: expert validation of LLM feedback vs. video review; target $\ge 85\%$.
\end{itemize}

\paragraph{Evaluation will use:}
\begin{itemize}
    \item Fit3D dataset — for offline benchmarking of segmentation and feature thresholds.
    \item Self-recorded data — for calibration and real-time tests.
\end{itemize}

\section{Expected Contributions}
\begin{itemize}
    \item Real-time, calibration-based extension of AIFit for interactive use.
    \item Lightweight AR feedback visualization framework requiring only a webcam.
    \item Integration of LLM-driven natural-language coaching with quantitative form analysis.
    \item Public demo and reproducible open-source code for academic use in mobile data analytics.
\end{itemize}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
